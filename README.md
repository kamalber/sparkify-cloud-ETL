## Project Overview:

"Sparkify's ETL Pipeline: Integrating AWS S3 Song and Log Datasets for Enhanced Music Streaming Analytics" - This project develops a comprehensive ETL pipeline for a music streaming startup, Sparkify. The aim is to facilitate data-driven insights by leveraging three key datasets stored in AWS S3, emphasizing precision and efficiency in data handling and transformation.

### Datasets Utilized:

***Song Dataset***: A subset from the [Million Song Dataset](http://millionsongdataset.com/) with detailed song and artist metadata, stored in JSON format.
***Log Dataset***: Simulated app activity logs, generated by an [event simulator](https://github.com/Interana/eventsim) in JSON format, reflect user interactions and are organized by year and month.
***Log JSON Paths***: Contains meta information for accurately loading the log data.

___

The objective of this project is to extensively retrieve JSON-formatted data from AWS S3 using Python, store it in an AWS Redshift data warehouse, and utilize SQL to transform the data into a structured star schema. This schema, which includes both fact and dimension tables, simplifies data analysis, making it easier to run SQL queries and retrieve important metrics like song plays, user activities, and other crucial data points. The star schema architecture is strategically designed to optimize SQL query performance and data aggregation, resulting in a strong framework for extracting valuable analytical insights in a cloud-based environment.


## Required Packages:

This project necessitates the installation of an external package, namely psycopg2.

```bash
$ pip install psycopg2
```

## Repository Contents:

**`create_tables.py`**
Responsible for the deletion and re-creation of database tables.

**`dwh.cfg`**
Manages settings for the Redshift cluster and oversees data import procedures.

**`etl.py`**
Facilitates the transfer of data into staging tables and populates the star schema's fact and dimension tables.

**`sql_queries.py`**
Oversees the creation and deletion of staging and star schema tables, handles the transfer of JSON-formatted data from Amazon S3 to Redshift staging tables, and ensures the insertion of data from these staging tables into the fact and dimension tables of the star schema.


## Execution Guidelines:

Set up the necessary environment variables by defining `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`. 

Update the dwh.cfg file to include the required variables.

Proceed to drop existing tables and recreate them:

```bash
$ python create_tables.py
```

Launch the ETL process:

```bash
$ python etl.py
```